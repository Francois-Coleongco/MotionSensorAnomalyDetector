{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "78647a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169 479\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dat.csv\")\n",
    "\n",
    "cols = [\"day_of_week\", \"hour_of_day\", \"motion_duration\"]\n",
    "X = df[cols].values\n",
    "y = df[\"label\"].values\n",
    "df.head()\n",
    "\n",
    "intrusions = df[df['label'] == 1]\n",
    "non_intrusions = df[df['label'] == 0]\n",
    "\n",
    "print(len(intrusions), len(non_intrusions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "8cfff4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to help make sure the model doesn't overfit, use early stopping when loss improves negligibly by delta\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=4, delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "bc1585a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "split_idx = int(len(X) * 0.8)\n",
    "\n",
    "X_train = X[:split_idx]\n",
    "y_train = y[:split_idx]\n",
    "X_test = X[split_idx:]\n",
    "y_test = y[split_idx:]\n",
    "\n",
    "# not using train_test_split because that apparently shuffles the data destroying the temporal nature of my data :/ not too bad  though. just slicey\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "b47dea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sliding window for LSTM timesteps per input, using a window of data to make a prediction on one piece\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "SEQ_LEN = 10\n",
    "def create_sequences(X, y, seq_len):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        X_seq.append(X[i:i+seq_len])\n",
    "        y_seq.append(y[i+seq_len])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, SEQ_LEN)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test, SEQ_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "3f122972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[379 129]\n",
      "[0.00263852 0.00775194]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_seq, dtype=torch.float32)\n",
    "\n",
    "class_counts = np.bincount(y_train_seq)\n",
    "print(class_counts)\n",
    "class_weights = 1.0 / class_counts\n",
    "print(class_weights)\n",
    "sample_weights = class_weights[y_train_seq]\n",
    "\n",
    "sampler=WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=16, sampler=sampler)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "bdbf96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Detector(nn.Module):\n",
    "    def __init__(self, input_size=3, hidden_size=32, num_layers=1, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                           batch_first=True, bidirectional=bidirectional) # perhaps make bidirectional to learn patterns relating to other motions\n",
    "        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        self.fc = nn.Linear(lstm_output_size, 1)\n",
    "        print(f\"creating Detector with hidden_size={hidden_size}, and num_layers={num_layers}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return torch.sigmoid(self.fc(out[:, -1, :])).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "bdfe467b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating Detector with hidden_size=32, and num_layers=1\n"
     ]
    }
   ],
   "source": [
    "model = Detector()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "698218b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopping() # trying with default paramss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "d9c50e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 | train loss: 0.6945 | val loss: 0.6904 | Val Acc: 68.33% | Val F1: 0.2692\n",
      "epoch 2 | train loss: 0.6909 | val loss: 0.7073 | Val Acc: 28.33% | Val F1: 0.4416\n",
      "epoch 3 | train loss: 0.6870 | val loss: 0.6997 | Val Acc: 28.33% | Val F1: 0.4416\n",
      "epoch 4 | train loss: 0.6820 | val loss: 0.6662 | Val Acc: 75.83% | Val F1: 0.4314\n",
      "epoch 5 | train loss: 0.6656 | val loss: 0.6338 | Val Acc: 82.50% | Val F1: 0.6957\n",
      "epoch 6 | train loss: 0.5600 | val loss: 0.5403 | Val Acc: 70.00% | Val F1: 0.5500\n",
      "epoch 7 | train loss: 0.4353 | val loss: 0.4749 | Val Acc: 77.50% | Val F1: 0.6747\n",
      "epoch 8 | train loss: 0.4048 | val loss: 0.4375 | Val Acc: 80.00% | Val F1: 0.7000\n",
      "epoch 9 | train loss: 0.3936 | val loss: 0.4644 | Val Acc: 78.33% | Val F1: 0.6286\n",
      "epoch 10 | train loss: 0.2998 | val loss: 0.4005 | Val Acc: 78.33% | Val F1: 0.6905\n",
      "epoch 11 | train loss: 0.2841 | val loss: 0.3299 | Val Acc: 82.50% | Val F1: 0.7200\n",
      "epoch 12 | train loss: 0.2768 | val loss: 0.4002 | Val Acc: 81.67% | Val F1: 0.7027\n",
      "epoch 13 | train loss: 0.2382 | val loss: 0.3275 | Val Acc: 87.50% | Val F1: 0.7619\n",
      "epoch 14 | train loss: 0.1833 | val loss: 0.3593 | Val Acc: 86.67% | Val F1: 0.7778\n",
      "epoch 15 | train loss: 0.1872 | val loss: 0.3074 | Val Acc: 88.33% | Val F1: 0.7941\n",
      "epoch 16 | train loss: 0.2063 | val loss: 0.3234 | Val Acc: 88.33% | Val F1: 0.8000\n",
      "epoch 17 | train loss: 0.2213 | val loss: 0.3294 | Val Acc: 89.17% | Val F1: 0.8169\n",
      "epoch 18 | train loss: 0.1740 | val loss: 0.4058 | Val Acc: 85.83% | Val F1: 0.7733\n",
      "early stop triggered!!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for epoch in range(400):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            all_preds.extend(output.round().numpy())\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "    \n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "    acc = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "\n",
    "    early_stopper(val_loss, model)\n",
    "\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"early stop triggered!!\")\n",
    "        break\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"epoch {epoch + 1} | train loss: {avg_train_loss:.4f} | val loss: {avg_val_loss:.4f} | Val Acc: {acc*100:.2f}% | Val F1: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
